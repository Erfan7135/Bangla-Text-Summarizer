{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. First create a tmp folder\n",
        "2. Then upload stopwords-bn.txt file\n"
      ],
      "metadata": {
        "id": "UsJGpsdUN1iT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing necessary modules"
      ],
      "metadata": {
        "id": "sPIEg91UuiTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import itertools\n",
        "import collections\n",
        "import editdistance\n",
        "import networkx as nx\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4w_zpFeuf41",
        "outputId": "86ceadd7-c757-413f-fa42-ff3b1ce778eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenizing from a text."
      ],
      "metadata": {
        "id": "FDqlP_2u-9U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class sentTokenizing:\n",
        "    def sentTokenize(self,gettingText):\n",
        "        dataToReSize=[]\n",
        "        data=[]\n",
        "        cleanText=''\n",
        "        for i in gettingText:\n",
        "            if i=='।' or i=='!' or i=='?':\n",
        "                cleanText+=i\n",
        "                dataToReSize.append(''.join(cleanText))\n",
        "                cleanText=''\n",
        "            else:\n",
        "                if i=='\\n' or i=='\\r' or i=='”' or i=='“' or i=='\"':\n",
        "                    continue\n",
        "                else:\n",
        "                    cleanText+=i\n",
        "        #print (dataToReSize)\n",
        "        for i in dataToReSize:\n",
        "            withoutAheadSpace=''\n",
        "            flag=1\n",
        "            for j in i:\n",
        "                if j==' ' and flag:\n",
        "                    continue\n",
        "                else:\n",
        "                    flag=0\n",
        "                    withoutAheadSpace+=j\n",
        "            data.append(''.join(withoutAheadSpace))\n",
        "        #print(data)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "bUylXT-CtjO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordConverter"
      ],
      "metadata": {
        "id": "s8raEBFbuyoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class word:\n",
        "    def sentToWord(self,gettingData):\n",
        "        cleanSent=''\n",
        "        for i in gettingData:\n",
        "            for j in i:\n",
        "                if j=='”' or j=='“' or j=='\"' or j==',' or j=='‘' or j=='’':\n",
        "                    cleanSent+=' '\n",
        "                    continue\n",
        "                elif j=='!' or j=='?' or j=='।':\n",
        "                    cleanSent+=' '\n",
        "                    continue\n",
        "                elif j=='(' or j=='{' or j=='}' or j=='[' or j==']':\n",
        "                    cleanSent+=' '\n",
        "                else:\n",
        "                    if j=='-' or j==':' or j==')':\n",
        "                        cleanSent+=' '\n",
        "                    else:\n",
        "                        cleanSent+=j\n",
        "        return nltk.word_tokenize(cleanSent)"
      ],
      "metadata": {
        "id": "2s_jhIEfuzIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textrank"
      ],
      "metadata": {
        "id": "giXQ90yBtqjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph(nodes):\n",
        "    \"\"\"Return a networkx graph instance.\n",
        "    :param nodes: List of hashables that represent the nodes of a graph.\n",
        "    \"\"\"\n",
        "    gr = nx.Graph()  # initialize an undirected graph\n",
        "    gr.add_nodes_from(nodes)\n",
        "    nodePairs = list(itertools.combinations(nodes, 2))\n",
        "\n",
        "    # add edges to the graph (weighted by Levenshtein distance)\n",
        "    for pair in nodePairs:\n",
        "        firstString = pair[0]\n",
        "        secondString = pair[1]\n",
        "        levDistance = editdistance.eval(firstString, secondString)\n",
        "        gr.add_edge(firstString, secondString, weight=levDistance)\n",
        "    return gr\n",
        "\n",
        "def extract_sentences(text, summary_length=40, clean_sentences=False, language='bangla'):\n",
        "    \"\"\"Return a paragraph formatted summary of the source text.\n",
        "    :param text: A string.\n",
        "    \"\"\"\n",
        "    sentence_tokens = sentTokenizing().sentTokenize(text)\n",
        "    graph = build_graph(sentence_tokens)\n",
        "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
        "\n",
        "    # most important sentences in ascending order of importance\n",
        "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
        "\n",
        "    # return a 100 word summary\n",
        "    summary = ' '.join(sentences)\n",
        "    summary_words = summary.split()\n",
        "    summary_words = summary_words[0:summary_length]\n",
        "    dot_indices = [idx for idx, word in enumerate(summary_words) if word.find('।') != -1]\n",
        "    #if clean_sentences and dot_indices:\n",
        "    if  dot_indices:\n",
        "        last_dot = max(dot_indices) + 1\n",
        "        summary = ' '.join(summary_words[0:last_dot])\n",
        "    else:\n",
        "        summary = ' '.join(summary_words)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def extract_summary(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        #summary = extract_key_phrases(f.read())\n",
        "        summary = extract_sentences(f.read())\n",
        "        return summary"
      ],
      "metadata": {
        "id": "EGWS8Y1vtpwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering"
      ],
      "metadata": {
        "id": "z8I1wdmrvym5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_only(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Zঅ-ঔং ৎ  ক-‍ঁ ]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens\n",
        "\n",
        "def hierarchicalClustering(data,pos,document):\n",
        "    key = 0\n",
        "    k = []\n",
        "    my_dict = dict()\n",
        "\n",
        "    silhouette_scores = []\n",
        "    for n in range(2,len(document)):\n",
        "        #Find the agglomerativeClustering\n",
        "        cluster1 = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
        "        y1 = cluster1.fit_predict(pos)\n",
        "        #Calculate Silhouette Scores\n",
        "        silhouette_scores.append(silhouette_score(pos, y1))\n",
        "        k.append(n)\n",
        "        my_dict[key]= n\n",
        "        key = key + 1\n",
        "    #Find the maximum Silhouette Score\n",
        "    maxx = silhouette_scores.index(max(silhouette_scores))\n",
        "\n",
        "    # This section is intentionally commented out, uncomment this if you want to see the graph of Silhouette Scores.\n",
        "    # Plotting a bar graph to compare the results\n",
        "    # plt.bar(k, silhouette_scores)\n",
        "    # plt.xlabel('Number of clusters', fontsize = 20)\n",
        "    # plt.ylabel('S(i)', fontsize = 20)\n",
        "    # plt.show()\n",
        "\n",
        "    #Get the no of cluster which have maximum Silhouette Score\n",
        "    val = list(my_dict.values())\n",
        "    n_clusters = val[maxx]\n",
        "\n",
        "    #Find the agglomerativeClustering\n",
        "    cluster1 = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
        "    y1 = cluster1.fit_predict(pos)\n",
        "\n",
        "    #Create file\n",
        "    p = str(n_clusters)\n",
        "    f = open(\"tmp/cluster\"+p+\".txt\",\"w\", encoding='utf-8')\n",
        "    f.truncate(0)\n",
        "    f = open(\"tmp/cluster\"+p+\".txt\",\"a+\", encoding='utf-8')\n",
        "\n",
        "    #Save the clusters in the temporary file\n",
        "    clusters = collections.defaultdict(list)\n",
        "    for i, label in enumerate(cluster1.labels_):\n",
        "        clusters[label].append(i)\n",
        "    dict(clusters)\n",
        "\n",
        "    for cluster in range(n_clusters):\n",
        "        # print(\"cluster \",cluster,\":\")\n",
        "\n",
        "        for i,sentence in enumerate(clusters[cluster]):\n",
        "            # print(\"\\tsentence \",i,\": \",document[sentence])\n",
        "            f.write(document[sentence])\n",
        "            f.write(\" \")\n",
        "        f.write('\\n\\n')\n",
        "    return n_clusters\n",
        "\n",
        "def multidimensionalScaling(similarity_distance,document):\n",
        "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
        "    pos = mds.fit_transform(similarity_distance)  # shape (n_components, n_samples)\n",
        "    xs, ys = pos[:, 0], pos[:, 1]\n",
        "    n_clusters = hierarchicalClustering(similarity_distance,pos,document)\n",
        "    return n_clusters\n",
        "\n",
        "def similarityDistance(tfidf_matrix,document):\n",
        "    similarity_distance = 1 - cosine_similarity(tfidf_matrix)\n",
        "    n_clusters = multidimensionalScaling(similarity_distance,document)\n",
        "    return n_clusters\n",
        "\n",
        "def tfidf(document,stopword):\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_df=10, min_df=2, use_idf=True, tokenizer=tokenize_only, ngram_range=(1,3))\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(document)\n",
        "    terms = tfidf_vectorizer.get_feature_names_out()\n",
        "    n_clusters = similarityDistance(tfidf_matrix,document)\n",
        "    return n_clusters\n",
        "\n",
        "def peep(n):\n",
        "    p = str(n)\n",
        "    cluster_sentence1 = open(\"tmp/cluster\"+p+\".txt\", encoding='utf-8').read().split('\\n\\n')\n",
        "    cluster_sentence1.pop((len(cluster_sentence1))-1)\n",
        "    # print(\"Cluster Sentences are:\",cluster_sentence1)\n",
        "    filenamee1 = []\n",
        "    for j in range(n):\n",
        "        ab = str(j)\n",
        "        namee = \"tmp/Cluster\"+p+\".\"+ab+\".txt\"\n",
        "        filenamee1.append(namee)\n",
        "        with open(namee, 'w', encoding='utf-8') as f:\n",
        "            for item in cluster_sentence1[j]:\n",
        "                f.write(item)\n",
        "    return filenamee1, n\n",
        "\n",
        "def startF(document):\n",
        "    stopword = open('stopwords-bn.txt', 'r', encoding='utf-8').read().split('\\n')\n",
        "    n_clusters = tfidf(document,stopword)\n",
        "    filenamee1, n  = peep(n_clusters)\n",
        "    return filenamee1, n"
      ],
      "metadata": {
        "id": "uYlZ5K5fvywx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "b7TxG7f0vPbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def listToString(s):\n",
        "    str1 = \" \"\n",
        "    return (str1.join(s))\n",
        "\n",
        "#Sentence ordering\n",
        "def sentence_ordering(unordered_summary):\n",
        "    unordered_summary = [x[:(len(x)-1)].strip(' ') for x in unordered_summary]\n",
        "    docs = [x[:(len(x)-1)].strip(' ') for x in doc]\n",
        "    both = set(docs).intersection(unordered_summary)\n",
        "    indices_A = [docs.index(x) for x in both]\n",
        "    indices_B = [unordered_summary.index(x) for x in both]\n",
        "    dictionary = {}\n",
        "    for x in range(len(indices_A)):\n",
        "        dictionary[indices_A[x]] = indices_B[x]\n",
        "    ordered_summary = []\n",
        "    for i in sorted (dictionary) :\n",
        "        ordered_summary.append(dictionary[i])\n",
        "    st =\"\"\n",
        "    for i in ordered_summary:\n",
        "        st = st +(unordered_summary[i])+'।\\n'\n",
        "    return st\n",
        "\n",
        "def getSummary(filename):\n",
        "    summary = []\n",
        "    for k in range(n):\n",
        "        summary_lines = extract_summary(filename[k])\n",
        "        summary.append(summary_lines)\n",
        "    full_summary =[]\n",
        "    for x in summary:\n",
        "        left_text = x.partition(\"।\")[0]\n",
        "        left_text = left_text.partition(\"?\")[0]\n",
        "        left_text = left_text.partition(\"!\")[0]\n",
        "        full_summary.append(left_text+\"।\")\n",
        "    s = listToString(full_summary)\n",
        "    si = sentTokenizing().sentTokenize(s)\n",
        "    # print(si)\n",
        "    summ = sentence_ordering(si)\n",
        "    return summ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BchuzX0AvPzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize a single file"
      ],
      "metadata": {
        "id": "OgNh_QcSxPaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# document = open('input.txt', 'r', encoding='utf-8').read()\n",
        "document = \"বেনাপোলের অবকাঠামোগত উন্নয়নসহ আমদানি-রপ্তানি বাণিজ্যের গতি ফেরাতে বেনাপোল-যশোর মহাসড়ক ৬ লেনে উন্নতকরাসহ বাইপাস সড়কের প্রশস্তকরণ, পণ্য লোড আনলোডের সরঞ্জাম এবং জনবল বৃদ্ধির লক্ষে রবিবার বিকেলে সড়ক ও জনপথসহ বিশ্ব ব্যাংকের ১৭ সদস্যের একটি প্রতিনিধিদল বেনাপোল বন্দর ও চেকপোস্ট এলাকা পরিদর্শন করেছেন।এর আগে বেনাপোল বন্দর প্যাসেজ্ঞার টার্মিনালের কনফারেন্স রুমে বন্দর, কাস্টম ও সিএন্ডএফ এজেন্ট ও বন্দর ব্যবহারকারী বিভিন্ন সংগঠনের নেতা ও প্রশাসনের কর্মকর্তাদের সাথে মত বিনিময় করেন প্রতিনিধিদলটি। উন্নয়ন কর্মকাণ্ডের পূর্ব প্রস্ততি হিসেবে প্রতিনিধি দলের বেনাপোল পরিদর্শন বলে জানান বন্দর সংশ্লিষ্টরা।বাংলাদেশ সড়ক ও জনপথের অতিরিক্ত প্রধান প্রকৌশলী রেজা আহম্মেদ জাবের প্রতিনিধিদলের নেতৃত্ব দেন। এ সময় উপস্থিত ছিলেন, বিশ্ব ব্যাংকের প্রতিনিধি মুন্নি জাহান, বাংলাদেশ স্থলবন্দর কর্তৃপক্ষের তত্বাবধায়ক প্রকৌশলী মোহাম্মদ হাসান আলী, স্থলবন্দর বেনাপোল উপপরিচালক মামুন কবির তরফদার, বেনাপোল কাস্টমের সহকারী কমিশনার উত্তম কুমার চাকমা, বন্দরের সহকারী পরিচালক মেহেদী হাসান, বেনাপোল সিএন্ডএফ এজেন্ট এসোসিয়েশন সভাপতি মফিজুর রহমান সজনসহ ভারত, নেপাল, নাইজেরিয়াসহ বিভিন্ন দেশের প্রতিনিধিরা।\"\n",
        "doc = sentTokenizing().sentTokenize(document)\n",
        "filenamee, n = startF(doc)\n",
        "summary = getSummary(filenamee)\n",
        "print('Document : '+ document)\n",
        "print('Summary : ' + summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGjP53K2vrDb",
        "outputId": "8b164e31-87e6-4974-95c6-28bbd7c68355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document : বেনাপোলের অবকাঠামোগত উন্নয়নসহ আমদানি-রপ্তানি বাণিজ্যের গতি ফেরাতে বেনাপোল-যশোর মহাসড়ক ৬ লেনে উন্নতকরাসহ বাইপাস সড়কের প্রশস্তকরণ, পণ্য লোড আনলোডের সরঞ্জাম এবং জনবল বৃদ্ধির লক্ষে রবিবার বিকেলে সড়ক ও জনপথসহ বিশ্ব ব্যাংকের ১৭ সদস্যের একটি প্রতিনিধিদল বেনাপোল বন্দর ও চেকপোস্ট এলাকা পরিদর্শন করেছেন।এর আগে বেনাপোল বন্দর প্যাসেজ্ঞার টার্মিনালের কনফারেন্স রুমে বন্দর, কাস্টম ও সিএন্ডএফ এজেন্ট ও বন্দর ব্যবহারকারী বিভিন্ন সংগঠনের নেতা ও প্রশাসনের কর্মকর্তাদের সাথে মত বিনিময় করেন প্রতিনিধিদলটি। উন্নয়ন কর্মকাণ্ডের পূর্ব প্রস্ততি হিসেবে প্রতিনিধি দলের বেনাপোল পরিদর্শন বলে জানান বন্দর সংশ্লিষ্টরা।বাংলাদেশ সড়ক ও জনপথের অতিরিক্ত প্রধান প্রকৌশলী রেজা আহম্মেদ জাবের প্রতিনিধিদলের নেতৃত্ব দেন। এ সময় উপস্থিত ছিলেন, বিশ্ব ব্যাংকের প্রতিনিধি মুন্নি জাহান, বাংলাদেশ স্থলবন্দর কর্তৃপক্ষের তত্বাবধায়ক প্রকৌশলী মোহাম্মদ হাসান আলী, স্থলবন্দর বেনাপোল উপপরিচালক মামুন কবির তরফদার, বেনাপোল কাস্টমের সহকারী কমিশনার উত্তম কুমার চাকমা, বন্দরের সহকারী পরিচালক মেহেদী হাসান, বেনাপোল সিএন্ডএফ এজেন্ট এসোসিয়েশন সভাপতি মফিজুর রহমান সজনসহ ভারত, নেপাল, নাইজেরিয়াসহ বিভিন্ন দেশের প্রতিনিধিরা।\n",
            "Summary : উন্নয়ন কর্মকাণ্ডের পূর্ব প্রস্ততি হিসেবে প্রতিনিধি দলের বেনাপোল পরিদর্শন বলে জানান বন্দর সংশ্লিষ্টরা।\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize a csv file & save content summary into another csv file"
      ],
      "metadata": {
        "id": "iQCeGOoYOXDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "df = pd.read_csv('text_summarization_dataset2.csv')\n",
        "summary_df = pd.DataFrame(columns=['content', 'summary'])\n",
        "\n",
        "for i in range(0,df.size-1):\n",
        "  tmp = open('tmp_input.txt', 'w', encoding='utf-8')\n",
        "  content = df['content'][i]\n",
        "  tmp.write(content)\n",
        "  tmp.close()\n",
        "\n",
        "  document = open('tmp_input.txt', 'r', encoding='utf-8').read()\n",
        "  doc = sentTokenizing().sentTokenize(document)\n",
        "  try:\n",
        "    filenamee, n = startF(doc)\n",
        "\n",
        "    summary = getSummary(filenamee)\n",
        "\n",
        "    summary_df = summary_df.append({'content': content, 'summary': summary}, ignore_index=True)\n",
        "  except:\n",
        "    #continue\n",
        "    summary = None\n",
        "\n",
        "summary_df.to_csv('summary_dataset_2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "iIiNr23KAbN8",
        "outputId": "72d7c5cc-eced-4d42-cef1-57c49ce8bffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "105252",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 105252 is not in range",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-82f7f0a40eef>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tmp_input.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 105252"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df.size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYspUB3EI7X4",
        "outputId": "70ed91f3-3e13-4757-cbb3-db446fe9825c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180622"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_df.to_csv('summary_dataset_2.csv')"
      ],
      "metadata": {
        "id": "5qjyEbmdvPRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'].size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcbBMt1paduv",
        "outputId": "b949a377-d60d-493e-ac67-96dffef2939c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105252"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# print(df.head())\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# for i in range(df.size):\n",
        "print(df['content'][1])\n",
        "doc = sentTokenizing().sentTokenize(df['content'][1])\n",
        "print(doc)\n",
        "filenamee, n = startF(doc)\n",
        "summary = getSummary(filenamee)\n",
        "print('summary: ', summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON2RcIUP-jc5",
        "outputId": "10aa738b-e5a6-4112-ff75-daf847141d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয় (বুয়েট) আজ বৃহস্পতিবার থেকে অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করা হয়েছে। আজ বিকেল পাঁচটার মধ্যে আবাসিক হলে অবস্থানরত সব ছাত্র-ছাত্রীকে হল ছাড়ার নির্দেশ দেওয়া হয়েছে। আজ আড়াইটার দিকে এ আদেশ সংবলিত বিজ্ঞপ্তি বিভিন্ন হলের নোটিশ বোর্ডে সেঁটে দেওয়া হয়। বুয়েটের রেজিস্ট্রার অধ্যাপক এ কে এম মাসুদ স্বাক্ষরিত ওই বিজ্ঞপ্তির ভাষ্য, চলতি টার্মের পূর্ব ঘোষিত টার্ম ফাইনাল পরীক্ষা পেছানোর দাবিতে ২৩ জুন একদল ছাত্র-ছাত্রীর উপাচার্য, রেজিস্ট্রার ও ছাত্রকল্যাণ পরিচালককে উপাচার্য কার্যালয়ে জিম্মি করা, ২৪ জুন শিক্ষকদের আবাসিক এলাকা অবরুদ্ধ করা এবং রাতে একাডেমিক ভবন ভাঙচুর ও অগ্নিসংযোগ করার পরিপ্রেক্ষিতে বিশ্ববিদ্যালয়ের সার্বিক আইনশৃঙ্খলা পরিস্থিতির চরম অবনতি ঘটছে এবং শিক্ষার পরিবেশ ভীষণভাবে বিঘ্নিত হচ্ছে। এ অবস্থায় বিশ্ববিদ্যালয়ের সার্বিক শৃঙ্খলা বজায় রাখা, ছাত্র-ছাত্রী, শিক্ষক ও কর্মকর্তা-কর্মচারীদের জানমালের নিরাপত্তা বিধানের স্বার্থে এবং শিক্ষার সুষ্ঠু পরিবেশ ফিরিয়ে আনার লক্ষ্যে এ বিশ্ববিদ্যালয়ের সব শিক্ষা কার্যক্রম আজ বিকেল থেকে অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করা হলো। রীক্ষা পেছানোর দাবিতে ‘বিশৃঙ্খলার’ প্রেক্ষাপটে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়-বুয়েট অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করেছে কর্তৃপক্ষ। সেই সঙ্গে বৃহস্পতিবার বিকাল ৫টার মধ্যে ছাত্র-ছাত্রীদের হল ছাড়ার নির্দেশ দেওয়া হয়েছে বলে বুয়েটের ছাত্রকল্যাণ পরিচালক অধ্যাপক দেলোয়ার হোসেন জানান। তিনি বলেন, \\\"রোজায় ছাত্রছাত্রীরা পরীক্ষা দিতে চাচ্ছিল না। এ কারণে তারা বিশ্ববিদ্যালয়ে ভাংচুর করে। অস্থিতিশীলত পরিস্থিতির সৃষ্টি হওয়ায় কর্তৃপক্ষ এই সিদ্ধান্ত নিয়েছে।\\\"\n",
            "\n",
            "\n",
            "\n",
            "['বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয় (বুয়েট) আজ বৃহস্পতিবার থেকে অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করা হয়েছে।', 'আজ বিকেল পাঁচটার মধ্যে আবাসিক হলে অবস্থানরত সব ছাত্র-ছাত্রীকে হল ছাড়ার নির্দেশ দেওয়া হয়েছে।', 'আজ আড়াইটার দিকে এ আদেশ সংবলিত বিজ্ঞপ্তি বিভিন্ন হলের নোটিশ বোর্ডে সেঁটে দেওয়া হয়।', 'বুয়েটের রেজিস্ট্রার অধ্যাপক এ কে এম মাসুদ স্বাক্ষরিত ওই বিজ্ঞপ্তির ভাষ্য, চলতি টার্মের পূর্ব ঘোষিত টার্ম ফাইনাল পরীক্ষা পেছানোর দাবিতে ২৩ জুন একদল ছাত্র-ছাত্রীর উপাচার্য, রেজিস্ট্রার ও ছাত্রকল্যাণ পরিচালককে উপাচার্য কার্যালয়ে জিম্মি করা, ২৪ জুন শিক্ষকদের আবাসিক এলাকা অবরুদ্ধ করা এবং রাতে একাডেমিক ভবন ভাঙচুর ও অগ্নিসংযোগ করার পরিপ্রেক্ষিতে বিশ্ববিদ্যালয়ের সার্বিক আইনশৃঙ্খলা পরিস্থিতির চরম অবনতি ঘটছে এবং শিক্ষার পরিবেশ ভীষণভাবে বিঘ্নিত হচ্ছে।', 'এ অবস্থায় বিশ্ববিদ্যালয়ের সার্বিক শৃঙ্খলা বজায় রাখা, ছাত্র-ছাত্রী, শিক্ষক ও কর্মকর্তা-কর্মচারীদের জানমালের নিরাপত্তা বিধানের স্বার্থে এবং শিক্ষার সুষ্ঠু পরিবেশ ফিরিয়ে আনার লক্ষ্যে এ বিশ্ববিদ্যালয়ের সব শিক্ষা কার্যক্রম আজ বিকেল থেকে অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করা হলো।', 'রীক্ষা পেছানোর দাবিতে ‘বিশৃঙ্খলার’ প্রেক্ষাপটে বাংলাদেশ প্রকৌশল বিশ্ববিদ্যালয়-বুয়েট অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করেছে কর্তৃপক্ষ।', 'সেই সঙ্গে বৃহস্পতিবার বিকাল ৫টার মধ্যে ছাত্র-ছাত্রীদের হল ছাড়ার নির্দেশ দেওয়া হয়েছে বলে বুয়েটের ছাত্রকল্যাণ পরিচালক অধ্যাপক দেলোয়ার হোসেন জানান।', 'তিনি বলেন, \\\\রোজায় ছাত্রছাত্রীরা পরীক্ষা দিতে চাচ্ছিল না।', 'এ কারণে তারা বিশ্ববিদ্যালয়ে ভাংচুর করে।', 'অস্থিতিশীলত পরিস্থিতির সৃষ্টি হওয়ায় কর্তৃপক্ষ এই সিদ্ধান্ত নিয়েছে।']\n",
            "summary:  আজ বিকেল পাঁচটার মধ্যে আবাসিক হলে অবস্থানরত সব ছাত্র-ছাত্রীকে হল ছাড়ার নির্দেশ দেওয়া হয়েছে।\n",
            "আজ আড়াইটার দিকে এ আদেশ সংবলিত বিজ্ঞপ্তি বিভিন্ন হলের নোটিশ বোর্ডে সেঁটে দেওয়া হয়।\n",
            "এ অবস্থায় বিশ্ববিদ্যালয়ের সার্বিক শৃঙ্খলা বজায় রাখা, ছাত্র-ছাত্রী, শিক্ষক ও কর্মকর্তা-কর্মচারীদের জানমালের নিরাপত্তা বিধানের স্বার্থে এবং শিক্ষার সুষ্ঠু পরিবেশ ফিরিয়ে আনার লক্ষ্যে এ বিশ্ববিদ্যালয়ের সব শিক্ষা কার্যক্রম আজ বিকেল থেকে অনির্দিষ্টকালের জন্য বন্ধ ঘোষণা করা হলো।\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data.csv')\n",
        "new_df = pd.DataFrame(columns=['content', 'summary', 'bensumm'])\n",
        "for i in range(df.size):\n",
        "  try:\n",
        "    doc = sentTokenizing().sentTokenize(df['content'][i])\n",
        "    filenamee, n = startF(doc)\n",
        "    summary = getSummary(filenamee)\n",
        "    new_df = new_df.append({'content': df['content'][i], 'summary': df['summary'][i], 'bensumm': summary}, ignore_index=True)\n",
        "  except:\n",
        "    summary = None\n",
        "\n",
        "new_df.to_csv('bensumm_data.csv')"
      ],
      "metadata": {
        "id": "LtRRiFvBHpab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duDILCqLHsKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}